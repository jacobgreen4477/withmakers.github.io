<!DOCTYPE html>
<html>

<head>

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-55499LK');</script>
    <!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
	
    <!-- Favicon Icon -->
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <title>withmakers 비트코인 &amp; 부동산 데이터분석</title>
    <meta name="description"
          content="">

    <link rel="canonical" href="http://localhost:4000/search.html">
    <link rel="alternate" type="application/rss+xml" title="withmakers 비트코인 & 부동산 데이터분석" href="http://localhost:4000/feed.xml">

    <script type="text/javascript" src="/bower_components/jquery/dist/jquery.min.js"></script>

    <!-- Third-Party CSS -->
    <link rel="stylesheet" href="/bower_components/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="/bower_components/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="/bower_components/hover/css/hover-min.css">
    <link rel="stylesheet" href="/bower_components/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">

    <!-- My CSS -->
    <link rel="stylesheet" href="/assets/css/common.css">

    <!-- CSS set in page -->
    
    <link rel="stylesheet" href="/assets/css/index.css">
    
    <link rel="stylesheet" href="/assets/css/sidebar-popular-repo.css">
    

    <!-- CSS set in layout -->
    

    <script type="text/javascript" src="/bower_components/bootstrap/dist/js/bootstrap.min.js"></script>

</head>


    <body>
    
    <!-- Google Tag Manager (noscript) -->
	<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-55499LK"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->

    <style>
.dropdown {
    position: relative;
    display: inline-block;
}

.dropdown-content {
    display: none;
    position: absolute;
    background-color: #f1f1f1;
    min-width: 160px;
    box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
    z-index: 1;
}

.dropdown-content a {
    color: black;
    padding: 12px 16px;
    text-decoration: none;
    display: block;
}

.dropdown-content a:hover {background-color: #ddd;}

.dropdown:hover .dropdown-content {display: block;}

.dropdown:hover .dropbtn {background-color: #3e8e41;}
</style>

<header class="site-header" style="background-color:white; margin-top:0">
<!--<header class="site-header" style=background-color:white;>-->    
    
    <div class="container">
        <span id="site-header-brand">
            <a href="https://withmakers.github.io/"><img src="https://raw.githubusercontent.com/withmakers/withmakers.github.io/master/_img/withmakers.jpg" style="max-width:50%;max-height:50%;"></img></a>
        </span>

        <nav class="site-header-nav" role="navigation">
            

              
              <a href="/realestate"
                 class=" site-header-nav-item hvr-underline-from-center"
                 target=""
                 title="부동산"
                 style="color:black">
                  부동산
              </a>
               


              


              


            

              
              <a href="/bitcoin"
                 class=" site-header-nav-item hvr-underline-from-center"
                 target=""
                 title="비트코인"
                 style="color:black">
                  비트코인
              </a>
               


              


              


            
        </nav>
    </div>
</header>


        <div class="content">
            <section class="jumbotron" style="background-image: url(https://raw.githubusercontent.com/2econsulting/2econsulting.github.io/master/_img/nn.gif);background-repeat: no-repeat; background-size: 100% 100%;">
<!--<section class="jumbotron">-->   
    <div class="container">
        <h1>withmakers 비트코인 & 부동산 데이터분석</h1>
        <div id="jumbotron-meta-info">
            <span class="meta-info hvr-grow">
                <span class="octicon octicon-organization"></span>
                <a href="" target="_blank">DB손해보험</a>
            </span> 
            <span class="meta-info hvr-grow">
                <span class="octicon octicon-mark-github"></span>
                <a href="https://github.com/withmakers" target="_blank">깃허브</a>
            </span> 
            <!--
            <span class="meta-info hvr-grow">
                <span class="octicon octicon-plus"></span>
                <a href="https://2econsulting.github.io/subscribe" target="_blank">구독</a>
            </span>
          -->
            <span class="meta-info hvr-grow">
                <span class="octicon octicon-mail"></span>
                <a href="https://flaskapp2e.herokuapp.com" target="_blank">PFMS</a>
            </span>
            <span class="meta-info hvr-grow">
                <span class="octicon octicon-graph"></span>
                <a href="https://2econsulting.github.io/request/request" target="_blank">분석request</a>
            </span>
        </div>
    </div>
</section>

<section class="content container">

    <div class="row">

        <!-- Post List -->
        <div class="col-md-8">

          <form action="/search.html" method="get">
            <label for="search-box"></label>
            <input type="text" id="search-box" name="query">
            <input type="submit" value="검색">
          </form>

          <br>

          <ul id="search-results"></ul>

          <script>
            window.store = {
              
                "2019-02-25-realestate-tera-v101-html": {
                  "title": "2019년 2월 부동산 데이터 탐색 (테라펀딩)",
                  "author": "",
                  "category": "",
                  "content": "&lt;!DOCTYPE html&gt;[withmakers] TERA펀딩 데이터 분석    [withmakers] TERA펀딩 데이터 분석          Column샘플 데이터번호지역순위차수리파이낸싱신축다세대연립주택타운하우스건축자금담보NPL근질권ABL수익률투자기간등급모집금액투자인원TARGET1564용인13110010000012.423B2999121518연체1558용인12110010000012.423B21000001394연체1554용인11110010000012.423B21000001583연체1516가평112010100000012.703B212000159연체1470일산117011000000012.503B2700083연체1448가평111010100000012.704B2500068연체1411가평110010100000012.704B225000644연체1394일산116011000000012.503B212000223연체1355태안13111000000014.006C1840001494연체1342태안12111000000014.007C11000001737연체1331태안11111000000014.006C11000001907연체1240고양15011000000012.078B130000798연체1225가평19010100000012.705B215000383연체1176경기118011000000013.007C116000257연체1144가평18010100000012.706B220000393연체1118고양14011000000012.079B140000774연체1091경기117011000000013.008C110000140연체1035일산115011000000012.507B210000179연체1020고양12011000000012.0710B135500653연체1019제주12111000000012.206B11350002155연체1015제주11111000000012.206B11350001754연체1014고양11011000000012.0710B1800001532연체985경기116011000000013.0010C120000405연체965가평17010100000012.708B235000541연체964일산114011000000012.507B210000130연체887가평16010100000012.709B230000495연체879경기115011000000013.0011C125000534연체877일산113011000000012.508B210000123연체843일산112011000000012.509B2500098연체828일산111011000000012.509B215000349연체813가평15010100000012.7010B220000463연체796가평19010100000012.636B220000388연체774가평14010100000012.7010B220000307연체762일산110011000000012.5010B223000433연체754가평13010100000012.7010B210000243연체730경기114011000000013.0012C120000393연체697가평18010100000012.637B214000297연체673경기113011000000013.0013C15000135연체659일산19011000000012.5010B215000296연체650일산18011000000012.5010B25000142연체640일산17011000000012.5011B215000318연체625경기112011000000013.0013C15000132연체593경기111011000000013.0013C13000103연체588가평12010100000012.7011B234000695연체587경기110011000000013.0013C1400076연체583경기19011000000013.0014C1200057연체578경기18011000000013.0014C1200061연체577경기17011000000013.0014C1200035연체576가평11010100000012.7012B240000636연체566경기16011000000013.0014C1300096연체지역별 연체Column투자기간 연체수익률 연체모집금액 연체투자인원 연체등급 연체Column리파이낸싱 연체신축 연체다세대 연체연립주택 연체타운하우스 연체",
                  "url": "/2019/02/25/realestate-tera-v101.html"
                }
                ,
              
                "2019-02-23-bitcoin-prediction-v101-html": {
                  "title": "2019년 3월 비트코인 가격 예측",
                  "author": "",
                  "category": "",
                  "content": "         실시간 코인 가격&lt;!DOCTYPE html&gt;2019-02-23-bitcoin-prediction-v101                      &#48708;&#53944;&#53076;&#51064; &#50696;&#52769;&#47784;&#45944; &#49345;&#49464;&#182;모델 정보 : v1.0.1 타겟 : bitcoin 문제 : regression  데이터 : poloniex 데이터 학습 기간 : 2018년 1월 ~ 2019년 2월 프로그램 : Python 패키지 : sklearn 알고리즘 : RF, GBM, eXtra Trees, Bayesian Ridge, Elastic Net 기타 : 작업 중 모델 In&nbsp;[1]:    import numpy as npimport osimport pandas as pdimport urllib.requestimport datetime as dtimport statsmodels.api as smfrom statsmodels.tsa.seasonal import seasonal_decomposefrom statsmodels.tsa.stattools import adfullerimport plotly.offline as pyimport plotly.graph_objs as goimport plotly.figure_factory as ffpy.init_notebook_mode(connected=False)from sklearn import preprocessingfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressorfrom sklearn.linear_model import BayesianRidge, ElasticNetCVfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error     def GetAPIUrl(cur, sts = 1420070400):    &#39;&#39;&#39;    Makes a URL for querying historical prices of a cyrpto from Poloniex    cur:    3 letter abbreviation for cryptocurrency (BTC, LTC, etc)    &#39;&#39;&#39;    return &#39;https://poloniex.com/public?command=returnChartData&amp;currencyPair=USDT_{:s}&amp;start={:d}&amp;end=9999999999&amp;period=7200&#39;.format(cur, sts) def GetCurDF(cur):    &#39;&#39;&#39;    cur:    3 letter abbreviation for cryptocurrency (BTC, LTC, etc)    &#39;&#39;&#39;    openUrl = urllib.request.urlopen(GetAPIUrl(cur))    r = openUrl.read()    openUrl.close()    df = pd.read_json(r.decode())    return df # Different cryptocurrency types# cl = [&#39;BTC&#39;, &#39;LTC&#39;, &#39;ETH&#39;, &#39;XMR&#39;]cl = [&#39;BTC&#39;]# Columns of price data to useCN = [&#39;close&#39;, &#39;high&#39;, &#39;low&#39;, &#39;open&#39;, &#39;volume&#39;]        In&nbsp;[2]:    # download datadata = GetCurDF(&#39;BTC&#39;)data.tail()        Out[2]:                  close      date      high      low      open      quoteVolume      volume      weightedAverage                  17570      3966.300373      2019-02-22 22:00:00      3966.300373      3966.300373      3966.300373      0.000000      0.000000      3966.300373              17571      3966.300373      2019-02-23 00:00:00      3966.300373      3966.300373      3966.300373      0.000000      0.000000      3966.300373              17572      3966.300373      2019-02-23 02:00:00      3966.300373      3966.300373      3966.300373      0.000000      0.000000      3966.300373              17573      3966.300373      2019-02-23 04:00:00      3966.300373      3966.300373      3966.300373      0.000000      0.000000      3966.300373              17574      3955.711200      2019-02-23 06:00:00      3966.300373      3950.617266      3966.300373      20.899337      82675.941063      3955.912066      In&nbsp;[3]:    # data rangeprint(data.date.agg([min,max]))        min   2015-02-19 18:00:00max   2019-02-23 06:00:00Name: date, dtype: datetime64[ns]In&nbsp;[4]:    # data slicedata[&#39;ymd&#39;] = data.date.astype(str).str[:10]data = data[data[&#39;ymd&#39;]&gt;&#39;2018-01-01&#39;].copy()del data[&#39;ymd&#39;]print(data.date.agg([min,max]))        min   2018-01-02 00:00:00max   2019-02-23 06:00:00Name: date, dtype: datetime64[ns]In&nbsp;[5]:    # features data[&#39;oc_diff&#39;]=data[&#39;close&#39;]-data[&#39;open&#39;]data[&#39;daily_avg&#39;] = (data[&#39;open&#39;] + data[&#39;high&#39;] + data[&#39;low&#39;] + data[&#39;close&#39;]) / 4data[&#39;daily_avg_After_Month&#39;]=data[&#39;daily_avg&#39;].shift(-30)    In&nbsp;[6]:    data.index = data.datedata.head()del data[&#39;date&#39;]    In&nbsp;[7]:    # Bitcoin (BTC)BTC = data.copy()BTC[&#39;daily_avg_After_Month&#39;]=BTC[&#39;daily_avg&#39;].shift(-30)X_BTC = BTC.dropna().drop([&#39;daily_avg_After_Month&#39;,&#39;daily_avg&#39;], axis=1)y_BTC = BTC.dropna()[&#39;daily_avg_After_Month&#39;]X_train_BTC, X_test_BTC, y_train_BTC, y_test_BTC = train_test_split(X_BTC, y_BTC, test_size=0.2, random_state=43)X_forecast_BTC =  BTC.tail(30).drop([&#39;daily_avg_After_Month&#39;,&#39;daily_avg&#39;], axis=1)    In&nbsp;[8]:    # define regression functiondef regression(X_train, X_test, y_train, y_test):    Regressor = {        &#39;Random Forest Regressor&#39;: RandomForestRegressor(n_estimators=200),        &#39;Gradient Boosting Regressor&#39;: GradientBoostingRegressor(n_estimators=500),        &#39;ExtraTrees Regressor&#39;: ExtraTreesRegressor(n_estimators=500, min_samples_split=5),        &#39;Bayesian Ridge&#39;: BayesianRidge(),        &#39;Elastic Net CV&#39;: ElasticNetCV(cv=3)    }    for name, clf in Regressor.items():        print(name)        clf.fit(X_train, y_train)            print(f&#39;R2: {r2_score(y_test, clf.predict(X_test)):.2f}&#39;)        print(f&#39;MAE: {mean_absolute_error(y_test, clf.predict(X_test)):.2f}&#39;)        print(f&#39;MSE: {mean_squared_error(y_test, clf.predict(X_test)):.2f}&#39;)    In&nbsp;[9]:    # Bitcoin (BTC)print(&#39;Bitcoin (BTC):&#39;)regression(X_train_BTC, X_test_BTC, y_train_BTC, y_test_BTC)        Bitcoin (BTC):Random Forest RegressorR2: 0.96MAE: 327.71MSE: 257444.63Gradient Boosting RegressorR2: 0.95MAE: 338.61MSE: 270067.43ExtraTrees RegressorR2: 0.96MAE: 330.49MSE: 255009.09Bayesian RidgeR2: 0.95MAE: 371.23MSE: 302459.18Elastic Net CVR2: 0.36MAE: 1444.55MSE: 3722590.91In&nbsp;[10]:    # define prediction functiondef prediction(name, X, y, X_forecast):    if name in [&#39;XRP&#39;, &#39;LTC&#39;]:        model = RandomForestRegressor(n_estimators=200)    else:        model = ExtraTreesRegressor(n_estimators=500, min_samples_split=5)    model.fit(X, y)    target = model.predict(X_forecast)    return target    In&nbsp;[11]:    # calculate forecasted prices for next 30 daysforecasted_BTC = prediction(&#39;BTC&#39;, X_BTC, y_BTC, X_forecast_BTC)    In&nbsp;[12]:    # define index for next 30 dayslast_date=data.iloc[-1].namemodified_date = last_date + dt.timedelta(days=1)new_date = pd.date_range(modified_date,periods=30,freq=&#39;D&#39;)# assign prediction to newly defined indexforecasted_BTC = pd.DataFrame(forecasted_BTC, columns=[&#39;daily_avg&#39;], index=new_date)# combine historical price and predicted pricebitcoin = pd.concat([data[[&#39;daily_avg&#39;]], forecasted_BTC])    In&nbsp;[13]:    # ouputoutput = bitcoin[&#39;2019-01-02&#39;:].copy()output.head()        Out[13]:                  daily_avg                  2019-01-02 00:00:00      3784.518260              2019-01-02 02:00:00      3774.443932              2019-01-02 04:00:00      3773.964999              2019-01-02 06:00:00      3767.425138              2019-01-02 08:00:00      3778.456956      In&nbsp;[14]:    from plotly import __version__from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot# plotY = go.Scatter(    x=output.index[:-30],    y=output[:-30][&#39;daily_avg&#39;],    name = &quot;Historical Price&quot;,    line = dict(color = &#39;blue&#39;,width=1),    opacity = 0.8)Yhat = go.Scatter(    x=output.index[-31:],    y=output[-31:][&#39;daily_avg&#39;],    name = &quot;Predicted Price&quot;,    line = dict(color = &#39;red&#39;,dash=&#39;dot&#39;,width=1),    opacity = 0.8)data = [Y,Yhat]layout = dict(title=&#39;2019년 3월 비트코인 예측&#39;,xaxis = {&#39;tickformat&#39;: &#39;%Y-%m-%d&#39;})fig = dict(data=data, layout=layout)plot(fig,filename=&quot;2019-02-23-bitcoin-prediction-v101-plot.html&quot;)        Out[14]:&#39;file://C:\\\\Users\\\\user\\\\2019-02-23-bitcoin-prediction-v101-plot.html&#39;       ",
                  "url": "/2019/02/23/bitcoin-prediction-v101.html"
                }
                ,
              
                "kaggle-2018-08-31-kagglehomecredit2-html": {
                  "title": "(Kaggle Home Credit) top 1% with only 2 submissions? how?",
                  "author": "",
                  "category": "[&quot;Kaggle&quot;]",
                  "content": "learning from arnowaczynskihttps://www.kaggle.com/c/home-credit-default-risk/discussion/64609[[ 모델 블랜딩 전략 ]]      many CV runs with random seeds with different random split(10 folds)        simple average of top 30 models        ridge regression on top 60 models  [[ 하이퍼 파라미터 탐색 전략 ]]      num_boost_round = 10000 with early_stopping_rounds=200        bagging_freq = 1        tuning hyper-params(6):  learning_rate, num_leaves, max_depth, min_data_in_leaf, feature_fraction,  bagging_freq        random grid search        set discrete search space for each params        while searching, print every cv score with selected hyper-parameters        at any time, stop search and adjust the search space (make sure not same parames evaluated more than once)  ",
                  "url": "/kaggle/2018/08/31/kaggleHomecredit2.html"
                }
                ,
              
                "kaggle-2018-08-30-kagglesantander-html": {
                  "title": "Kaggle Santander Value Prediction TOP 8%",
                  "author": "",
                  "category": "[&quot;Kaggle&quot;]",
                  "content": "The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service. In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer’s transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer.[feature engineering]  leak features from kernel  statistical features (max, min, sd, median, avg)  na count features by row[modeling]  single light gbm  bayesian grid search  cv predict  boosting samples  cut range",
                  "url": "/kaggle/2018/08/30/kaggleSantander.html"
                }
                ,
              
                "kaggle-2018-08-30-kagglehomecredit-html": {
                  "title": "Kaggle Home Credit Default Risk TOP 6%",
                  "author": "",
                  "category": "[&quot;Kaggle&quot;]",
                  "content": "Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data–including telco and transactional information–to predict their clients’ repayment abilities.While Home Credit is currently using various statistical and machine learning methods to make these predictions, they’re challenging Kagglers to help them unlock the full potential of their data.[feature engineering]  collect many ideas from kernel  interaction features  indicator features[modeling]  many light gbm models with different dataset  bayesian grid search  cv predict  boosting samples  blend of base models",
                  "url": "/kaggle/2018/08/30/kaggleHomecredit.html"
                }
                ,
              
                "kaggle-2018-07-10-kaggletitanic-html": {
                  "title": "Kaggle Titanic Machine Learning from Disaster TOP 3%",
                  "author": "",
                  "category": "[&quot;Kaggle&quot;]",
                  "content": "캐글(Kaggle)은 전세계에서 가장 큰 데이터 분석 경진 대회입니다.https://www.kaggle.com/  캐글은 2010년 설립된 예측모델 및 분석 대회 플랫폼이다. 기업 및 단체에서 데이터와 해결과제를 등록하면, 데이터 과학자들이 이를 해결하는 모델을 개발하고 경쟁한다. 2017년 3월 구글에 인수되었다. (위키피디아)Titanic Competition 소개Titanic Competition은 데이터 분석 입문자들을 위한 캐글 데이터 분석 대회입니다.Titanic Competition은 1912년 4월 15일 타이타닉 침몰 사고의 실제 데이터 입니다.데이터셋에는 객실 등급, 나이, 성별, 항만, 티켓번호 이름 등의 정보가 있고 이를 사용해서 승객의 생존을 예측하는 경진 대회입니다.DS랩도 Titanic Competition에 참여헸고 결과는 다음과 같습니다.사용한 머신러닝 알고리즘Titanic Competition 경진대회에서 사용한 분석 방법은 다음과 같습니다. (RANK TOP 3%)  사용한 머신러닝 알고리즘 : CatBoost, H2O XGBoost, H2O Light GBM  CatBoost, (H2O) XGBoost, (H2O) Light GBM in R and Python 모델 튜닝 코드 (https://github.com/2econsulting/Kaggle)            file name      accuracy      threshold                  R_TEST_CATBOOST_TUNE_P1.csv      0.82296      &gt;0.5              R_TEST_CATBOOST_TUNE_CVP1.csv      0.81818      &gt;0.5              R_TEST_H2OXGB_TUNE_CVP1.csv      0.81339      &gt;0.5              R_TEST_CATBOOST_TUNE_CVP1_78.csv      0.81339      &gt;0.5              R_TEST_H2OLGB_TUNE_CVP1.csv      0.81818      &gt;0.5      단일 모델로는 CatBoot의 0.82296(AUC)으로 가장 높았습니다.CatBoost, H2O XGBoost, H2O Light GBM의 예측값을 다수결 방식으로 결합한 Stacking 모델은 0.83253(AUC)을 얻었습니다.",
                  "url": "/kaggle/2018/07/10/kaggleTitanic.html"
                }
                ,
              
                "featuretools-python-2018-06-29-featuretools-html": {
                  "title": "Featuretools(automating feature engineering) in Python",
                  "author": "",
                  "category": "[&quot;Featuretools&quot;, &quot;Python&quot;]",
                  "content": "연구 중, 업로드 예정일 2018년 9월 30일",
                  "url": "/featuretools/python/2018/06/29/featuretools.html"
                }
                ,
              
                "lightgbm-2018-06-25-gbm-vs-lightgbm-html": {
                  "title": "Light GBM in R and Python (GBM vs Light GBM)",
                  "author": "",
                  "category": "LightGBM",
                  "content": "Light GBM은 2014년 3월에 나온 XGBoost 알고리즘 이후, 2017년 1월에 발표된 알고리즘이다. GBM은 가지가 1번 분리될 때 2개씩 매번 분리가 되서 overfitting이 잘 일어난다. 하지만 LightGBM은 가지가 1번 분리(2개의 node 생성)될 때  모든 가지를 분리하지 않고 2개의 node 중 잘 맞는 node 기준으로만 나눈다. 전체적인 아키텍처는 다음과 같다.Light GBM 구조XGBoost 구조XGBoost와 GBM은 둘 다 위와 같은 구조로 되어 있다. 따라서 node가 많이 생기므로, 모델이 더 복잡해진다. 복잡해진 모델은 overfitting을 야기하기 쉽다. 반면, LightGBM은 그림과 같이 node가 비교적 적게 생긴다. 따라서 GBM이나 XGBoost보다 overfitting이 일어날 확률이 더 적어진다.본 포스팅에서는 R과 Python 각각 GBM과 Light GBM을 비교해본다. 데이터는 “churn” 데이터이고 출처는 데이터 출처(Github) 이다. 현재 H2O로 Light GBM 모델링은 XGBoost에 parameter를 조정해서 사용한다. 하지만 현재 XGBoost 모듈은 Windows 에는 지원을 하지 않으니, 아래에 있는 코드를 돌리기 위해서는 Mac OS 나 linux 등 Windows 이외의 환경에서 실행을 해야 한다.성능비교 (GBM vs Light GBM in R &amp; Python)            model      time(sec)      AUC      Logloss                  H2O GBM in python      0.98      0.9118      0.1954              H2O Light GBM in python      2.31      0.9234      0.1694              H2O GBM in R      2.85      0.9118      0.1954              H2O Light GBM in R      3.91      0.9234      0.1694      전체 분석 코드  GBM vs Light GBM in R  GBM vs Light GBM in Python",
                  "url": "/lightgbm/2018/06/25/GBM_vs_LightGBM.html"
                }
                ,
              
                "catboost-2018-06-17-tuningcatboost-html": {
                  "title": "CatBoost 튜닝 in R (using caret패키지)",
                  "author": "",
                  "category": "CatBoost",
                  "content": "caret패키지를 사용한 CatBoost 튜닝 v0.01 (random grid search)# library library(catboost)library(caret)library(titanic)# load datadata &lt;- as.data.frame(as.matrix(titanic::titanic_train), stringsAsFactors=TRUE)# handle missing valueage_levels &lt;- levels(data$Age)most_frequent_age &lt;- which.max(table(data$Age))data$Age[is.na(data$Age)] &lt;- age_levels[most_frequent_age]# set x and y drop_columns = c(\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\")x &lt;- data[,!(names(data) %in% drop_columns)]y &lt;- data[,c(\"Survived\")]# use caret for grid search fit_control &lt;- caret::trainControl(  method = \"cv\",   number = 3,   search = \"random\",  classProbs = TRUE)# set grid optionsgrid &lt;- expand.grid(  depth = c(4, 6, 8),  learning_rate = 0.1,  l2_leaf_reg = 0.1,  rsm = 0.95,  border_count = 64,  iterations = 10)# train catboostmodel &lt;- caret::train(  x = x,   y = as.factor(make.names(y)),  method = catboost.caret,  metric = \"Accuracy\",  maximize = TRUE,  preProc = NULL,  tuneGrid = grid,   tuneLength = 30,   trControl = fit_control)print(model)# variable importanceimportance &lt;- varImp(model, scale = FALSE)print(importance)",
                  "url": "/catboost/2018/06/17/tuningCatBoost.html"
                }
                ,
              
                "catboost-2018-06-16-catboostsimpleversion-html": {
                  "title": "CatBoost in R &amp; Python (simple version)",
                  "author": "",
                  "category": "CatBoost",
                  "content": "새로운 머신러닝 알고리즘 CatBoost가 등장했습니다. 러시아 과학자가 개발한 CatBoost는 Tree Boosting 계열의 최신 머신러닝 알고리즘 입니다. 최근 들어 Tree Boosting 계열의 머신러닝 알고리즘이 활발하게 연구되고 있습니다. 이는 아마도 XGBoost가 캐글 대회에서 수차례 winning solution으로 검증되면서 많은 과학자들이 XGBoost와 같은 Tree Boosting 머신러닝 기법에 많은 관심을 갖고 있는 것 같습니다.Tree Boosting 머신러닝 변천사CatBoost의 장점특히 CatBoost의 full name은 Categorical Boost으로 범주형 변수가 많은 데이터셋에서 예측 성능이 우수하다고 합니다. link  높은 예측 성능  범주형 변수를 자동으로 전처리  모델 튜닝이 간소화 (범주형 변수를 자동으로 전처리 해주니깐 그 부분에 대해서 따로 튜닝을 할 필요x)  R 그리고 Python과 연동CatBoost의 장점 (추가)  CatBoost 개발자에 의하면 모델 튜닝 없이 default값으로만 좋은 성능을 보여준다고 합니다. 또한 튜닝을 통해서 얻을 수 있는 효과는 크지 않다고 합니다. link  CatBoost gives great results with default values of the training parameters. In most cases parameter tuning does not significantly affect the resulting quality of the model and therefore is unnecessary. However, CatBoost provides a very flexible interface for parameter tuning and can be configured to suit different tasks.성능비교 (RF vs GBM vs CatBoost)Tree기반의 대표적인 머신러닝 알고리즘에는 Random Forest(RF)와 Gradient Boosting Machine(GBM)이 존재합니다. CatBoost가 RF와 GBM과 비교해서 속도 및 예측 성능의 차이를 비교하였습니다.            model      time(sec)      AUC      Logloss                  H2ORF in Python      0.73      0.9153      0.3039              H2OGBM in Python      0.49      0.9118      0.1954              CatBoost in Python      22.50      0.9539      0.1148              H2ORF in R      1.57      0.9170      0.3607              H2OGBM in R      1.69      0.9160      0.1931              CatBoost in R      29.73      0.9259      0.1565      CatBoost 설치 방법 및 전체 분석 코드  CatBoost in Python  CatBoost in R",
                  "url": "/catboost/2018/06/16/catboostSimpleVersion.html"
                }
                ,
              
                "recommendersystem-2018-06-14-recommendersystem-html": {
                  "title": "추천 시스템 R패키지 비교 연구",
                  "author": "",
                  "category": "recommenderSystem",
                  "content": "추천 R패키지 속도 및 성능 비교 연구 결과입니다.  데이터셋 : MovieLense (100만행)  컴퓨터 사양 : 7i, 32Gb RAM패키지 목록            package name      package description                  Myrrix      Real-Time, Scalable Clustering and Recommender System, Evolved from Apache Mahout              recommenderlab      Lab for Developing and Testing Recommender Algorithms              recosystem      Recommender System using Matrix Factorization              rrecsys      Environment for Evaluating Recommender Systems              slimrec      Sparse Linear Method to Predict Ratings and Top-N Recommendations      패키지 성능 비교            package name      algorithm      time(min)      RMSE                  recommenderlab      Most Popular      4.27      0.9725                     User-Based CF      5.03      1.0464                     Item-based CF      7.11      1.5074                     SVD      5.52      1.0204                     Funk SVD      13.91      0.9106                     Random      3.49      1.3832                     ALS      13.14      0.9032              rrecsys      itemAverage      7.37      0.9614                     userAverage      6.95      1.0140                     globalAverage      6.22      1.0913                     IBKNN      7.53      1.0853                     UBKNN      37.49      1.0196                     FunkSVD      31.36      1.0811                     SlopeOne      15.48      0.9028              recosystem      Matrix Factorization      0.68      0.8529              slimrec      Sparse Linear Method      25.52      2.2196              SVDApproximation      SVDApproximation      4.92      0.9313              SmartCat-labs’s Git R code      ibcf      1.76      0.8859                     ubcf      1.74      0.8564      전체 분석 코드  Comparison of Recommend system packages in R",
                  "url": "/recommendersystem/2018/06/14/recommenderSystem.html"
                }
                ,
              
                "catboost-2018-06-14-catboost-html": {
                  "title": "CatBoost in R &amp; Python (detail)",
                  "author": "",
                  "category": "CatBoost",
                  "content": "Yandex 가 2017년 6월에 논문 CatBoost: unbiased boosting with categorical features에서 소개한 방법이다. CatBoost는 “Categorical Boost” 약자로, 이름에서부터 범주형 변수를 위한 Boosting 방법이라는 냄새를 풍기고 있다.저자는 기존에 범주형 변수를 변환했던 방법 중에, “Target mean encoding”로 하면 정보유출(target leakage)이 생긴다고 주장한다. Target mean encoding이란, 범주별로 target 비율을 구해 해당 값으로 대체하는 것이다. 예를 들어, X 변수에 {A, A, A, B, B}가 있고, Target이 {1, 0, 0, 1, 0}이면, A 범주에서 Target 비율은 0.3333, B 범주에서 Target 비율은 0.5 이므로, {0.3333, 0.3333, 0.3333, 0.5, 0.5}로 변환이 된다. 또한, 정보유출로 인해 “conditional shift”가 발생하는데, 다시말해서 train 데이터와 test 데이터가 다른 분포를 가진다는 것이다. 서로 다른 분포를 가진다면, train 데이터에서 학습 된 모형이 test 데이터에 잘 맞기가 힘들 것이다. 개선방안으로 다음 2 step을 제안한다.  Ordered TBS(Target-Based Statistics)  Ordered boosting먼저, Ordered TBS에 대해서 알아보자. 흥미로운 아이디어는, 변환하기 전에 데이터를 랜덤하게 섞는다는 것이다. 섞은 데이터에서 다음 식을 이용해 변환을 한다.여기서 대괄호([]) 의미는, 랜덤하게 재배치한 후 위에서부터 차례로 내려오면서 계산을 한다고 생각하면 된다. 이런 수식을 말로만 풀면 어려우니, Yandex 홈페이지에 있는 변환 예시를 보면서 설명한다.위에 예시에서 이 우리가 최종적으로 변환해야 할 범주형 변수이고, 데이터는 한번 랜덤으로 재배치가 끝난 상태이다. 간단하게 만들기 위해  로 놓고 다음과 같이 바꿔보자.       : 현재 행에 있는 범주 기준으로 이전까지 target ‘1’(or True) 갯수         : 미리 지정. (예시에서는 0.05)         : 현재 행에 있는 범주 기준으로 이전까지 해당 범주 갯수  이를 반영하면 아래와 같이 변경된다. 아래에 행마다 설명을 달아놓았으니 참고해서 이해하면 더 쉬울거라 생각한다.설명  rock : 처음등장. 0.05  indie : 처음증장. 0.05  rock :  (rock 기준 이전에 target = 0번, rock = 1번 등장)  rock  :  (rock 기준 이전에 target = 1번, rock = 2번 등장)  pop : 처음등장. 0.05  indie :  (indie 기준 이전에 target = 0번, indie = 1번 등장)  rock :  (rock 기준 이전에 target = 2번, rock = 3번 등장)후에 설명드릴 내용은, Ordered Boosting에 대한 내용이다. 이는 overfitting 을 방지하기 위한 방법으로 다소 복잡해 본 포스팅에서는 간단한 개념만 설명한다. 기존 Boosting 방법은 만약 번째 tree를 나누었다면 나눈 후에 오차를 계산해 가중치 업데이트를 했다. 하지만 이렇게 오차를 구하면 편향이 발생돼 train 데이터에만 잘 맞을 수 있다는 단점이 있다(overfitting). 왜냐하면 번 째 tree를 나눴을 때의 오차가 train과 test에서 같다고 볼 수 없기 때문이다. 따라서 저자는 논문에서, 학습시에 번 째 관측치는 제외한 까지의 관측으로 오차를 구해 “불편 잔차(unbiased residual)”를 구하고 가중치를 업데이트해야 overfitting을 방지할 수 있다고 주장한다.논문의 핵심 주제인 Ordered TBS와 Ordered Boosting에 대해서 간단히 알아보았다.장단점장점으로는, 범주형 변수를 변환할 때 기존 방법대비 정보유출을 덜 했다는 점이다. 어쨋든 target을 가지고 변환을 했기 때문에 정보유출이 아예 되지 않았다고 하기는 곤란하다. 하지만 순열로 데이터의 배열을 재배치한 후 인코딩 작업을 여러번 반복하면서 변환을 했다는 점이 흥미롭고 충분한 장점이 된다고 생각한다. 다만, 데이터가 n개가 있을 시 순열로 가능한 경우의 수는 (n factorial)이다. n이 100만 되어도,  이라는 어마어마한 경우의 수가 나온다. encoding을 할 때 배열된 순서에 따라 달라지기 때문에, 과연 경우의수를 몇번 뽑아서 학습을 해야 괜찮은지에 대한 기준이 애매하다는 점이 단점이다.실제 샘플데이터(Churn)로 CatBoost, Random Forest, GBM을 비교 결과 CatBoost가 속도가 상당히 느렸다. 전부 default로 된 parameter를 이용했는데, CatBoost는 20초대, 다른 2개는 1초 근방으로 거의 20배 차이가 난다. 이는 CatBoost가 default iteration이 1000번으로 설정 되어있기 때문이다. 경우의 수가 워낙 방대하기 때문에, 어느정도 많이 돌려야 결과를 신뢰할 수 있다. 따라서 일부러 큰 숫자로 설정한 것으로 추정된다. 자세한 코드 및 설명은 맨 아래 링크에 첨부되어 있다.Python      설치 방법    pip install catboost        학습 (예측성능, 속도) 비교 (아래 Jupyter notebook 첨부)    CatBoost in Python  R  설치방법          아래 Jupyter notebook 참고            학습 (예측성능, 속도) 비교 (아래 Jupyter notebook 첨부)    CatBoost in R  ",
                  "url": "/catboost/2018/06/14/catboost.html"
                }
                ,
              
                "gridsearch-2018-06-14-bayesiangridsearch-html": {
                  "title": "Bayesian Optimization using R and H2O",
                  "author": "",
                  "category": "gridSearch",
                  "content": "rBayesianOptimization을 사용한 h2o.gbm 튜닝 코드입니다. 모델 튜닝은 최적의 하이퍼 파라미터를 찾는 행위이고 이를 grid search라고 합니다.가장 많이 사용되고 있는 grid search 방법으로는 모든 가능한 경우를 테스트하는 Cartesian grid search방법과, 모든 가능한 경우 중 일부를 랜덤하게 선택하여 테스트하는 Random grid search방법이 존재합니다.Bayesian grid search방법은 기존의 Random, Cartesian방법 보다 더 고급진 grid search방법입니다.  Bayesian Optimization helped us find a hyperparameter configuration that is better than the one found by Random Search for a neural network on the San Francisco Crimes dataset. linkBayesian grid search의 특징은 최적의 하이퍼 파라미터를 찾는데 이전의 탐색 결과를 참고합니다. 이전에 탐색 결과를 다음 탐색에 참고하기 때문에 랜덤하게 탐색하는 것보다 더 효율적이라고 말할 수 있습니다.# bayesGridFunbayesGridFun &lt;- function(max_depth, min_rows, sample_rate, col_sample_rate){  gbm &lt;- h2o.gbm(    x = x, y = y, seed = 1234,    training_frame = h2o.rbind(train_hex, valid_hex),    nfolds = 3,    score_each_iteration = TRUE,    stopping_metric = \"logloss\",    ntrees = 10000,    stopping_rounds = autoGBM_BestParams$stopping_rounds,    stopping_tolerance = autoGBM_BestParams$stopping_tolerance,    categorical_encoding = autoGBM_BestParams$Random_categorical_encoding,    learn_rate = autoGBM_BestParams$Random_learn_rate,    max_depth = max_depth,    min_rows = min_rows,    sample_rate = sample_rate,    col_sample_rate = col_sample_rate  )  score &lt;- h2o.auc(gbm, xval = T)  list(Score = score, Pred  = 0)}# bayesGridOptionsmax_depth &lt;- autoGBM_BestParams$Random_max_depthsample_rate &lt;- autoGBM_BestParams$Random_sample_ratecol_sample_rate &lt;- autoGBM_BestParams$Random_col_sample_ratemin_rows &lt;- autoGBM_BestParams$Random_min_rowsbayesGridOptions &lt;- list(  max_depth = as.integer(c(max(2, max_depth-1), max_depth+1)),  min_rows  = as.integer(c(max(1, min_rows-5), min_rows+5)),  sample_rate = c(sample_rate-0.1, min(sample_rate+0.1, 1)),  col_sample_rate = c(col_sample_rate-0.1, min(col_sample_rate+0.1, 1)))# bayesGridSearchset.seed(1234)bayesGridSearch &lt;- rBayesianOptimization::BayesianOptimization(  FUN = bayesGridFun,  bounds = bayesGridOptions,  init_points = init_points,  n_iter = n_iter,  acq = \"ucb\",  kappa = 2.576,  eps = 0.0,  verbose = TRUE)# H2OGBM_BayesianH2OGBM_Bayesian &lt;- h2o.gbm(  x = x, y = y, seed = 1234,  model_id = \"H2OGBM_Bayesian\",  training_frame = train_hex,  validation_frame = valid_hex,  score_each_iteration = TRUE,  stopping_metric = \"logloss\",  ntrees = 10000,  stopping_rounds = autoGBM_BestParams$stopping_rounds,  stopping_tolerance = autoGBM_BestParams$stopping_tolerance,  categorical_encoding = autoGBM_BestParams$Random_categorical_encoding,  learn_rate = autoGBM_BestParams$Random_learn_rate,  max_depth = as.numeric(bayesGridSearch$Best_Par[\"max_depth\"]),  min_rows = as.numeric(bayesGridSearch$Best_Par[\"min_rows\"]),  sample_rate = as.numeric(bayesGridSearch$Best_Par[\"sample_rate\"]),  col_sample_rate = as.numeric(bayesGridSearch$Best_Par[\"col_sample_rate\"]))autoGBM_Models[\"H2OGBM_Bayesian\"] &lt;- list(h2o.getModel(\"H2OGBM_Bayesian\"))h2o.auc(h2o.performance(autoGBM_Models[\"H2OGBM_Bayesian\"][[1]], newdata = test_hex))saveRDS(autoGBM_Models['H2OGBM_Bayesian'], file.path(model_path, \"H2OGBM_Bayesian.rda\"))autoGBM_BestParams['Bayes_max_depth'] &lt;- as.numeric(bayesGridSearch$Best_Par[\"max_depth\"])autoGBM_BestParams['Bayes_min_rows'] &lt;- as.numeric(bayesGridSearch$Best_Par[\"min_rows\"])autoGBM_BestParams['Bayes_sample_rate'] &lt;- as.numeric(bayesGridSearch$Best_Par[\"sample_rate\"])autoGBM_BestParams['Bayes_col_sample_rate'] &lt;- as.numeric(bayesGridSearch$Best_Par[\"col_sample_rate\"])",
                  "url": "/gridsearch/2018/06/14/bayesianGridSearch.html"
                }
                ,
              
                "machineleraning-2018-06-13-rautoml-html": {
                  "title": "자동머신러닝(rAutoML) R패키지 개발",
                  "author": "",
                  "category": "MachineLeraning",
                  "content": "데이터 분석은 1) 데이터 전처리 영역과 2) 모델링 영역으로 구분할 수 있습니다. DS랩에서 개발한 rAutoML(Auto Machine Learning in R)은 모델링 영역을 자동화 해주는 R패키지 입니다. 현재(2018-06) rAutoML에서 제공하는 머신러닝 알고리즘은 H2OGBM이고 추후 H2ORF, H2OXGB 등 추가 될 예정입니다.rAutoML의  학습 프로세스는 다음과 같습니다.  H2OGBM_Default : 디폴트 모형 생성  H2OGBM_StopRules : 학습 스탑 규칙 결정 (Cartesian Grid Search)  H2OGBM_CatEncode : 범주형 변수의 최적의 전처리 방법 탐색 (Cartesian Grid Search)  H2OGBM_MaxDepth : max_depth의 최적의 범위 탐색 (Cartesian Grid Search)  H2OGBM_Random : learn_rate, sample_rate, col_sample_rate, min_rows의 최적값 탐색  (Random Grid Search)  H2OGBM_Bayesian : 소수점 단위로 정밀 튜닝 (Bayesian Grid Search)  H2OGBM_Default -&gt; H2OGBM_StopRules -&gt; H2OGBM_CatEncode -&gt; H2OGBM_MaxDepth -&gt; H2OGBM_Random -&gt; H2OGBM_BayesianrAutoML패키기를 사용한 GBM모형 튜닝 결과            models      auc      logloss                  H2OGBM_Default      0.9136564      0.2006527              H2OGBM_StopRules      0.8995600      0.2097254              H2OGBM_CatEncode      0.9154616      0.1566602              H2OGBM_MaxDepth      0.9166058      0.1754864              H2OGBM_Random      0.9225640      0.1522216              H2OGBM_Bayesian      0.9262237      0.1591523      학습 데이터 소개  churn dataset  3333 rows  Y is binary, X consists of 15 numeric and 4 categorical features  출처 : yhat (https://github.com/yhat/demo-churn-pred/blob/master/model/churn.csv)rAutoML패키지 설치 방법library(devtools)install_github(\"2econsulting/rAutoML\")library(rAutoML)",
                  "url": "/machineleraning/2018/06/13/rAutoML.html"
                }
                ,
              
                "featureselection-2018-06-13-rautofs-html": {
                  "title": "자동변수선택(rAutoFS) R패키지 개발",
                  "author": "",
                  "category": "featureSelection",
                  "content": "h2o.automl(h2o 3.20)는 여러 기초 모델을 생성하고 생성 된 기초 모델을 앙상블하여 보다 우수한 예측 모델을 생성하는 h2o패키지 함수입니다. 여기서 기초 모델을 앙상블할 경우 예측 성능이 높아지는데 이 경우 모델 위에 모델을 쌓는 방식이기 때문에 변수 중요도를 산출할 수 없다는 단점이 존재합니다.rAutoFS(Auto Feature Selection in R)는 이러한 단점을 보완하는 차원에서 개발되었습니다. rAutoFS는 h2o.automl을 사용하여 변수 중요도를 산출합니다. 변수 중요도 산출 로직은 심플합니다. 기초 모델의 변수 중요도를 종합하여 변수 종합 순위를 도출하고 이를 기반으로 변수 중요도를 산출합니다.  h2o.automl학습을 통해서 다수의 예측 모델 생성  생성 된 예측 모델에서 예측 성능이 높은 TOP N개의 모델을 선택 (num_of_model)  TOP N개의 모델의 변수 중요도 도출  도출 된 변수 중요도을 기준으로 vote방식으로 h2o.automl의 변수 중요도 종합rAutoFS::autoFS의 변수 중요도 결과는 다음과 같습니다. (패키지 example)            rank      vi_GBM14      vi_GBM0      vi_GBM1      vi_GBM23      vi_GBM55      vi                  1      Day.Charge      State      State      State      State      State              2      State      Day.Charge      Day.Charge      Day.Charge      Day.Charge      Day.Charge              3      CustServ.Calls      CustServ.Calls      CustServ.Calls      Day.Mins      Eve.Mins      CustServ.Calls              4      Int.l.Plan      Day.Mins      Day.Mins      CustServ.Calls      Int.l.Plan      Int.l.Plan              5      Day.Mins      Eve.Mins      Eve.Mins      Eve.Mins      Night.Mins      Eve.Mins              6      Intl.Charge      Int.l.Plan      Int.l.Plan      Intl.Calls      Intl.Calls      Int.l.Plan              7      Eve.Charge      Intl.Calls      Intl.Calls      Int.l.Plan      Night.Calls      Intl.Calls              8      VMail.Plan      Eve.Charge      Intl.Charge      Intl.Charge      CustServ.Calls      Intl.Charge              9      Intl.Calls      VMail.Message      Eve.Charge      Eve.Charge      Intl.Charge      Eve.Charge              10      Eve.Mins      Intl.Charge      VMail.Message      Intl.Mins      Day.Mins      Eve.Mins              11      Night.Mins      Intl.Mins      Intl.Mins      VMail.Plan      Account.Length      Intl.Mins              12      Night.Charge      VMail.Plan      VMail.Plan      Day.Calls      Eve.Calls      VMail.Plan              13      Intl.Mins      Night.Mins      Night.Mins      Night.Charge      Night.Charge      Night.Mins              14      Eve.Calls      Night.Charge      Night.Charge      VMail.Message      Intl.Mins      Night.Charge              15      Account.Length      Night.Calls      Account.Length      Night.Calls      Day.Calls      Account.Length              16      Night.Calls      Day.Calls      Night.Calls      Night.Mins      VMail.Plan      Night.Calls              17      Day.Calls      Account.Length      Day.Calls      Account.Length      Eve.Charge      Day.Calls              18      VMail.Message      Eve.Calls      Eve.Calls      Eve.Calls      Area.Code      Eve.Calls              19      Area.Code      Area.Code      Area.Code      Area.Code      VMail.Message      Area.Code      rAutoFS 설치 방법library(devtools)install_github(\"jacobgreen1984/rAutoFE\")library(rAutoFE)rAutoFS 실행 방법library(rAutoFS)library(h2o)h2o.init()data(churn, package = \"rAutoFS\")data_hex &lt;- as.h2o(churn)y = \"Churn.\"x = colnames(data_hex)[colnames(data_hex)!=y]autoFS(data_hex, x, y, num_of_model=5, num_of_vi=10)",
                  "url": "/featureselection/2018/06/13/rAutoFS.html"
                }
                ,
              
                "featureengineering-2018-06-13-rautofe-html": {
                  "title": "자동변수생성(rAutoFE) R패키지 개발",
                  "author": "",
                  "category": "featureEngineering",
                  "content": "데이터 분석에서 파생변수를 생성하는 과정은 매우 중요하고 노력과 시간이 많이 필요한 작업입니다.  We spent most of our efforts in feature engineering.  – Xavier Cohort, after winning one of many Kaggle competitions  … some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.  – Pedro Domingos, A Few Useful Things to Know about Machine LearningrAutoFE 소개rAutoFE(auto feature engineering in R)는 다양한 변수를 자동으로 생성해주는 R패키지 입니다.rAutoFE 파생변수 목록  numeric feature transformation  numeric feature binning  categorical feature encoding (frequency-based)  categorical feature encoding (target-based)  weight of evidence features  interaction features  reduce the number of levels for catogorical featuresrAutoFE 패키지 설치 방법library(devtools)install_github(\"2econsulting/rAutoFE\")library(rAutoFE)rAutoFE 패키지 사용 방법library(data.table)library(rAutoFE)library(rAutoFS)library(h2o)savePath &lt;- \"c:/tmp\"dir.create(savePath)data(churn)churn &lt;- as.data.table(churn)churn[, Area.Code:=as.factor(Area.Code)]splits &lt;- splitFrame(dt=churn, ratio = c(0.5, 0.2), seed=1234)train &lt;- splits[[1]]valid &lt;- splits[[2]]test  &lt;- splits[[3]]y = \"Churn.\"x = colnames(train)[colnames(train)!=y]h2o.init()dataset &lt;- autoFE(train=train, valid=valid, test=test, x=x, y=y, savePath=savePath, verbose=TRUE)",
                  "url": "/featureengineering/2018/06/13/rAutoFE.html"
                }
                ,
              
                "github-2018-06-13-makegithubblog-html": {
                  "title": "깃허브 블로그 만드는 방법",
                  "author": "",
                  "category": "github",
                  "content": "깃허브 계정과 연동되는 블로그 생성 방법을 공유합니다.      git 설치 https://gitforwindows.org/        ruby 설치 (jekyll 설치 하려면 필요)https://rubyinstaller.org/downloads/        jekyll 설치 gem install jekyll        github페이지에서 새로운 repository 생성, 이름은 반드시 아래의 형식을 따라야 함 [깃허브유저명].github.io  예. 2econsulting.github.io    로컬 컴퓨터 특정 폴더(C:\\Users\\jacob\\Documents\\GitHub\\2econsulting\\2econsulting.github.io)에 clone 하기    cd C:\\Users\\jacob\\Documents\\GitHub\\2econsultinggit clone https://github.com/2econsulting/2econsulting.github.io.git        jekyll themes 중에서 하나 선택 후 테마파일(zip) 해당 폴더에 다운로드http://jekyllthemes.org/    다운로드 파일 압축 풀고 commit &amp; push하면 깃허브 블러그 생성 완료    git statusgit add .git commit -m \"post upload\"git push origin master     기타, 블로그에 댓글 기능 추가http://recoveryman.tistory.com/391?category=635733",
                  "url": "/github/2018/06/13/makeGithubBlog.html"
                }
                ,
              
                "github-2018-06-13-howtowritepost-html": {
                  "title": "깃허브 블로그 글 쓰는 방법",
                  "author": "",
                  "category": "github",
                  "content": "깃허브 블로그 폴더(2econsulting.github.io/_posts/2018)에 마크다운 파일을 작성해서 저장합니다.파일 저장 포멧은 다음과 같습니다. “yyyy-mm-dd-파일명.markdown” 예. 2018-06-13-sample.markdown파일 저장 후, commit &amp; push하면 글이 업로드 됩니다.$git status$git add .$git commit -m \"post upload\"$git push origin master ",
                  "url": "/github/2018/06/13/howToWritePost.html"
                }
                ,
              
                "github-2018-06-12-shell-command-html": {
                  "title": "shell command",
                  "author": "",
                  "category": "github",
                  "content": "",
                  "url": "/github/2018/06/12/shell-command.html"
                }
                
              
            };
          </script>

          <script src="/assets/js/lunr.min.js"></script>
          <script src="/assets/js/search.js"></script>

        </div>

        <div class="col-md-4" style="left:5%">
            <h4 class="sidebar-title">깃허브 저장소</h4>


<div class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="bitcoin">
            <div class="card-image-cell">
                <h3 class="card-title">
                    <a href="https://github.com/withmakers/bitcoin" target="_blank">bitcoin</a>
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span data-toggle="tooltip" class="meta-info" title="0 stars">
                    <span class="octicon octicon-star"></span> 0
                </span>
                <span data-toggle="tooltip" class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span data-toggle="tooltip" class="meta-info" title="Last updated：2019-02-02 02:13:31 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2019-02-02 02:13:31 UTC" title="2019-02-02 02:13:31 UTC">2019-02-02</time>
                </span>
            </div>
        </div>
    </div>
</div>

<div class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="withmakers.github.io">
            <div class="card-image-cell">
                <h3 class="card-title">
                    <a href="https://github.com/withmakers/withmakers.github.io" target="_blank">withmakers.github.io</a>
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span data-toggle="tooltip" class="meta-info" title="0 stars">
                    <span class="octicon octicon-star"></span> 0
                </span>
                <span data-toggle="tooltip" class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span data-toggle="tooltip" class="meta-info" title="Last updated：2019-02-26 04:27:45 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2019-02-26 04:27:45 UTC" title="2019-02-26 04:27:45 UTC">2019-02-26</time>
                </span>
            </div>
        </div>
    </div>
</div>

<div class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="realestate">
            <div class="card-image-cell">
                <h3 class="card-title">
                    <a href="https://github.com/withmakers/realestate" target="_blank">realestate</a>
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span data-toggle="tooltip" class="meta-info" title="0 stars">
                    <span class="octicon octicon-star"></span> 0
                </span>
                <span data-toggle="tooltip" class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span data-toggle="tooltip" class="meta-info" title="Last updated：2019-02-02 02:13:31 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2019-02-02 02:13:31 UTC" title="2019-02-02 02:13:31 UTC">2019-02-02</time>
                </span>
            </div>
        </div>
    </div>
</div>

<script>
    $(document).ready(function(){

        // Enable bootstrap tooltip
        $("body").tooltip({ selector: '[data-toggle=tooltip]' });

        $('.geopattern').each(function(){
            $(this).geopattern($(this).data('pattern-id'));
        });

    });
</script>
        </div>

    </div>

</section>
        </div>

    <footer class="container">

    <div class="site-footer">

        <div class="pull-right">
            <a href="javascript:window.scrollTo(0,0)" >TOP</a>
        </div>

    </div>

    <!-- Third-Party JS -->
    <script type="text/javascript" src="/bower_components/geopattern/js/geopattern.min.js"></script>

    <!-- My JS -->
    <script type="text/javascript" src="/assets/js/script.js"></script>

    

    

</footer>


    </body>

</html>
